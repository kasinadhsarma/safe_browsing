{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Safe Browsing for Kids Under Parental Supervision Using Machine Learning\n",
        "\n",
        "This notebook contains the implementation of a safe browsing system for kids using machine learning for URL classification and deep learning for image detection."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Introduction\n",
        "### Problem Statement\n",
        "Children in the 21st century have easy access to the internet, but not all content is suitable for them. Exposure to inappropriate content can negatively impact their development. This project aims to provide a safe browsing environment for kids using machine learning and deep learning techniques.\n",
        "\n",
        "### Objectives\n",
        "- Classify URLs as safe or unsafe using supervised machine learning.\n",
        "- Detect inappropriate images using deep learning.\n",
        "- Provide parental controls with customizable settings for different age groups.\n",
        "\n",
        "### Key Features\n",
        "1. Parental control settings for different age groups.\n",
        "2. Real-time URL classification using ensemble ML models.\n",
        "3. Image content detection using deep learning.\n",
        "4. Detailed activity logging and reporting.\n",
        "5. Customizable risk thresholds for different age groups."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Collection and Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.utils import resample\n",
        "import joblib\n",
        "import os\n",
        "from urllib.parse import urlparse\n",
        "import datetime\n",
        "from dataset import generate_dataset, extract_url_features\n",
        "\n",
        "# Set style for plots\n",
        "plt.style.use('seaborn')\n",
        "sns.set_palette('husl')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def balance_dataset(X, y):\n",
        "    \"\"\"Balance dataset using upsampling for minority class\"\"\"\n",
        "    X_df = pd.DataFrame(X)\n",
        "    X_df['target'] = y\n",
        "\n",
        "    # Separate majority and minority classes\n",
        "    majority = X_df[X_df.target == 0]\n",
        "    minority = X_df[X_df.target == 1]\n",
        "\n",
        "    # Upsample minority class\n",
        "    minority_upsampled = resample(minority,\n",
        "                                  replace=True,\n",
        "                                  n_samples=len(majority),\n",
        "                                  random_state=42)\n",
        "\n",
        "    # Combine majority class with upsampled minority class\n",
        "    df_balanced = pd.concat([majority, minority_upsampled])\n",
        "\n",
        "    # Separate features and target\n",
        "    y_balanced = df_balanced.target\n",
        "    X_balanced = df_balanced.drop('target', axis=1)\n",
        "\n",
        "    return X_balanced.values, y_balanced.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate dataset\n",
        "print(\"Generating dataset...\")\n",
        "dataset = generate_dataset()\n",
        "\n",
        "# Extract features and labels\n",
        "feature_columns = [col for col in dataset.columns if col not in ['url', 'is_blocked', 'category']]\n",
        "X = dataset[feature_columns].values\n",
        "y = dataset['is_blocked'].values\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Balance training data\n",
        "X_train_balanced, y_train_balanced = balance_dataset(X_train, y_train)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
        "X_test_scaled = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. URL Classification Using Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.title(f'Confusion Matrix - {model_name}')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.show()\n",
        "\n",
        "def evaluate_model(y_true, y_pred, model_name):\n",
        "    \"\"\"Evaluate model and plot metrics\"\"\"\n",
        "    metrics = {\n",
        "        'Accuracy': accuracy_score(y_true, y_pred),\n",
        "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
        "        'F1 Score': f1_score(y_true, y_pred, zero_division=0)\n",
        "    }\n",
        "    \n",
        "    # Plot metrics\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(metrics.keys(), metrics.values())\n",
        "    plt.title(f'Performance Metrics - {model_name}')\n",
        "    plt.ylim(0, 1)\n",
        "    for i, v in enumerate(metrics.values()):\n",
        "        plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
        "    plt.show()\n",
        "    \n",
        "    # Plot confusion matrix\n",
        "    plot_confusion_matrix(y_true, y_pred, model_name)\n",
        "    \n",
        "    return metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize and train models\n",
        "models = {\n",
        "    'KNN': KNeighborsClassifier(n_neighbors=5, weights='distance'),\n",
        "    'SVM': SVC(kernel='rbf', probability=True, C=1.0),\n",
        "    'Naive Bayes': GaussianNB()\n",
        "}\n",
        "\n",
        "trained_models = {}\n",
        "model_predictions = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    print(f\"\\nTraining {name} model...\")\n",
        "    model.fit(X_train_scaled, y_train_balanced)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test_scaled)\n",
        "    model_predictions[name] = y_pred\n",
        "    \n",
        "    # Evaluate and plot\n",
        "    print(f\"\\n{name} Model Performance:\")\n",
        "    metrics = evaluate_model(y_test, y_pred, name)\n",
        "    \n",
        "    trained_models[name] = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Image Content Detection Using Deep Learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import pytorch_lightning as pl\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models, transforms\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_dir):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "        ])\n",
        "        \n",
        "        self.classes = ['safe', 'unsafe']\n",
        "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
        "        \n",
        "        self.samples = []\n",
        "        for class_name in self.classes:\n",
        "            class_dir = os.path.join(data_dir, class_name)\n",
        "            if os.path.exists(class_dir):\n",
        "                for img_name in os.listdir(class_dir):\n",
        "                    if img_name.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
        "                        img_path = os.path.join(class_dir, img_name)\n",
        "                        self.samples.append((img_path, self.class_to_idx[class_name]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\n",
        "def load_dataset(data_dir):\n",
        "    return ImageDataset(data_dir)\n",
        "\n",
        "# Define image classifier\n",
        "class ImageClassifier(pl.LightningModule):\n",
        "    def __init__(self, num_classes=2):\n",
        "        super().__init__()\n",
        "        self.model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        for param in self.model.parameters():\n",
        "            param.requires_grad = False\n",
        "        num_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Sequential(\n",
        "            nn.Linear(num_features, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(512, num_classes),\n",
        "            nn.Softmax(dim=1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = nn.CrossEntropyLoss()(y_hat, y)\n",
        "        preds = torch.argmax(y_hat, dim=1)\n",
        "        acc = (preds == y).float().mean()\n",
        "        self.log('train_loss', loss)\n",
        "        self.log('train_acc', acc)\n",
        "        return loss\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        optimizer = torch.optim.Adam(self.model.fc.parameters(), lr=0.001)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=3)\n",
        "        return {\"optimizer\": optimizer, \"lr_scheduler\": {\"scheduler\": scheduler, \"monitor\": \"train_loss\"}}\n",
        "\n",
        "# Train image classifier\n",
        "def train_image_classifier(train_data_dir, val_data_dir=None):\n",
        "    train_dataset = load_dataset(train_data_dir)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
        "    val_loader = None\n",
        "    if val_data_dir:\n",
        "        val_dataset = load_dataset(val_data_dir)\n",
        "        val_loader = DataLoader(val_dataset, batch_size=32, num_workers=4)\n",
        "    model = ImageClassifier()\n",
        "    trainer = pl.Trainer(max_epochs=20, accelerator='auto', devices=1)\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Parental Control Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Age group-based risk thresholds\n",
        "age_thresholds = {\n",
        "    'kid': 0.5,\n",
        "    'teen': 0.6,\n",
        "    'adult': 0.7\n",
        "}\n",
        "\n",
        "# Real-time URL and image blocking\n",
        "def predict_url(url, age_group='kid'):\n",
        "    features = extract_url_features(url)\n",
        "    features_array = np.array(list(features.values())).reshape(1, -1)\n",
        "    features_scaled = scaler.transform(features_array)\n",
        "    probabilities = {}\n",
        "    for name, model in trained_models.items():\n",
        "        probabilities[name] = model.predict_proba(features_scaled)[0][1]\n",
        "    final_risk_score = np.mean(list(probabilities.values()))\n",
        "    risk_level = 'High' if final_risk_score > age_thresholds[age_group] else 'Low'\n",
        "    return risk_level, final_risk_score\n",
        "\n",
        "# Activity logging\n",
        "activity_log = []\n",
        "\n",
        "def log_activity(url, risk_level, age_group):\n",
        "    activity_log.append({\n",
        "        'URL': url,\n",
        "        'Risk Level': risk_level,\n",
        "        'Age Group': age_group,\n",
        "        'Time': datetime.datetime.now().strftime(\"%b %d, %Y, %I:%M %p\")\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Save Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save models and preprocessing objects\n",
        "save_dir = 'models/latest'\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "for name, model in trained_models.items():\n",
        "    filename = name.lower().replace(' ', '_')\n",
        "    joblib.dump(model, os.path.join(save_dir, f'{filename}_model.pkl'))\n",
        "\n",
        "# Save scaler and feature columns\n",
        "joblib.dump(scaler, os.path.join(save_dir, 'url_scaler.pkl'))\n",
        "joblib.dump(feature_columns, os.path.join(save_dir, 'feature_cols.pkl'))\n",
        "\n",
        "print(\"Models saved successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare model performance\n",
        "metrics_df = pd.DataFrame()\n",
        "\n",
        "# Use trained_models instead of models since it contains the actual trained models\n",
        "for name in trained_models.keys():\n",
        "    y_pred = model_predictions[name]\n",
        "    metrics = {\n",
        "        'Model': name,\n",
        "        'Accuracy': accuracy_score(y_test, y_pred),\n",
        "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
        "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
        "        'F1 Score': f1_score(y_test, y_pred, zero_division=0)\n",
        "    }\n",
        "    metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics])], ignore_index=True)\n",
        "\n",
        "# Plot comparison\n",
        "metrics_melted = pd.melt(metrics_df, \n",
        "                        id_vars=['Model'], \n",
        "                        value_vars=['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
        "                        var_name='Metric', \n",
        "                        value_name='Score')\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.barplot(data=metrics_melted, x='Model', y='Score', hue='Metric')\n",
        "plt.title('Model Performance Comparison')\n",
        "plt.ylim(0, 1)\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test URL Classification with Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_age_based_risk(predictions, features, age_group='kid'):\n",
        "    \"\"\"Calculate risk level and score based on age group and features\"\"\"\n",
        "    # Base weights for different models based on their performance\n",
        "    model_weights = {\n",
        "        'knn': 0.3,\n",
        "        'svm': 0.4,\n",
        "        'nb': 0.3\n",
        "    }\n",
        "\n",
        "    # Calculate weighted average of model predictions\n",
        "    weighted_score = sum(pred['probability'] * model_weights[model] \n",
        "                        for model, pred in predictions.items())\n",
        "\n",
        "    # Age-specific risk modifiers\n",
        "    age_risk_multipliers = {\n",
        "        'kid': 1.5,    # More strict for kids\n",
        "        'teen': 1.2,   # Moderately strict for teens\n",
        "        'adult': 1.0   # Base level for adults\n",
        "    }\n",
        "\n",
        "    # Apply age-specific risk multiplier\n",
        "    risk_score = min(1.0, weighted_score * age_risk_multipliers.get(age_group, 1.0))\n",
        "\n",
        "    # Determine risk level\n",
        "    if risk_score > 0.8:\n",
        "        risk_level = 'high'\n",
        "    elif risk_score > 0.5:\n",
        "        risk_level = 'medium'\n",
        "    else:\n",
        "        risk_level = 'low'\n",
        "\n",
        "    return risk_level, risk_score\n",
        "\n",
        "def predict_url(url, threshold=0.65, models_dir=None, age_group='kid'):\n",
        "    \"\"\"Make prediction for a single URL using ensemble of models\"\"\"\n",
        "    if models_dir is None:\n",
        "        models_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'models', 'latest')\n",
        "\n",
        "    # Load models and scaler\n",
        "    knn = joblib.load(os.path.join(models_dir, 'knn_model.pkl'))\n",
        "    svm = joblib.load(os.path.join(models_dir, 'svm_model.pkl'))\n",
        "    nb = joblib.load(os.path.join(models_dir, 'naive_bayes_model.pkl'))\n",
        "    scaler = joblib.load(os.path.join(models_dir, 'url_scaler.pkl'))\n",
        "    feature_cols = joblib.load(os.path.join(models_dir, 'feature_cols.pkl'))\n",
        "\n",
        "    # Extract and prepare features\n",
        "    features = extract_url_features(url)\n",
        "    if isinstance(features, dict):\n",
        "        features = list(features.values())\n",
        "    features_array = np.array(features).reshape(1, -1)\n",
        "    features_scaled = scaler.transform(features_array)\n",
        "\n",
        "    # Get predictions\n",
        "    predictions = {}\n",
        "    probas = []\n",
        "\n",
        "    # KNN\n",
        "    knn_prob = knn.predict_proba(features_scaled)[0][1]\n",
        "    predictions['knn'] = {\n",
        "        'prediction': knn_prob > threshold,\n",
        "        'probability': knn_prob\n",
        "    }\n",
        "    probas.append(knn_prob)\n",
        "\n",
        "    # SVM\n",
        "    svm_prob = svm.predict_proba(features_scaled)[0][1]\n",
        "    predictions['svm'] = {\n",
        "        'prediction': svm_prob > threshold,\n",
        "        'probability': svm_prob\n",
        "    }\n",
        "    probas.append(svm_prob)\n",
        "\n",
        "    # Naive Bayes\n",
        "    nb_prob = nb.predict_proba(features_scaled)[0][1]\n",
        "    predictions['nb'] = {\n",
        "        'prediction': nb_prob > threshold,\n",
        "        'probability': nb_prob\n",
        "    }\n",
        "    probas.append(nb_prob)\n",
        "\n",
        "    # Calculate base risk from model predictions\n",
        "    base_features = extract_url_features(url) if isinstance(url, str) else dict(zip(feature_cols, features))\n",
        "\n",
        "    # Trust factors (reduce risk score)\n",
        "    trust_score = 1.0\n",
        "    if base_features.get('has_https', 0) == 1:\n",
        "        trust_score *= 0.7  # Significant trust for HTTPS\n",
        "\n",
        "    # Check for trusted domains\n",
        "    domain = urlparse(url).netloc.lower()\n",
        "    trusted_domains = {'github.com', 'python.org', 'wikipedia.org'}\n",
        "    if any(td in domain for td in trusted_domains):\n",
        "        trust_score *= 0.5  # High trust for known good domains\n",
        "\n",
        "    # Risk factors (increase risk score)\n",
        "    risk_multiplier = 1.0\n",
        "    if base_features.get('is_ip_address', 0) == 1:\n",
        "        risk_multiplier *= 2.0  # Major increase for IP-based URLs\n",
        "    if base_features.get('suspicious_word_count', 0) > 2:\n",
        "        risk_multiplier *= 1.5  # Increase for multiple suspicious words\n",
        "    if base_features.get('suspicious_tld', 0) == 1:\n",
        "        risk_multiplier *= 1.8  # Increase for suspicious TLDs\n",
        "\n",
        "    # Calculate age-specific thresholds\n",
        "    age_thresholds = {\n",
        "        'kid': 0.5,    # More strict for kids\n",
        "        'teen': 0.6,   # Moderate for teens\n",
        "        'adult': 0.7   # More lenient for adults\n",
        "    }\n",
        "    effective_threshold = age_thresholds.get(age_group, threshold)\n",
        "\n",
        "    # Get base risk score from models\n",
        "    risk_level, risk_score = calculate_age_based_risk(\n",
        "        predictions,\n",
        "        base_features,\n",
        "        age_group\n",
        "    )\n",
        "\n",
        "    # Apply trust and risk modifiers\n",
        "    final_risk_score = (risk_score * risk_multiplier * trust_score)\n",
        "\n",
        "    # Ensure score stays in [0,1] range\n",
        "    final_risk_score = max(0.0, min(1.0, final_risk_score))\n",
        "\n",
        "    # Update risk level based on final score\n",
        "    if final_risk_score > 0.8:\n",
        "        risk_level = 'high'\n",
        "    elif final_risk_score > 0.5:\n",
        "        risk_level = 'medium'\n",
        "    else:\n",
        "        risk_level = 'low'\n",
        "\n",
        "    # Enhanced result with age-specific risk assessment\n",
        "    result = {\n",
        "        'is_unsafe': bool(final_risk_score > effective_threshold),\n",
        "        'risk_score': final_risk_score,\n",
        "        'risk_level': risk_level,\n",
        "        'age_group': age_group,\n",
        "        'model_predictions': predictions\n",
        "    }\n",
        "\n",
        "    # Visualize the predictions\n",
        "    visualize_risk_assessment(url, predictions, final_risk_score, age_group)\n",
        "\n",
        "    return result['is_unsafe'], result['risk_score'], result['risk_score']\n",
        "\n",
        "def visualize_risk_assessment(url, probabilities, final_risk_score, age_group):\n",
        "    \"\"\"Create detailed visualization of URL risk assessment\"\"\"\n",
        "    fig = plt.figure(figsize=(15, 5))\n",
        "    \n",
        "    # Plot 1: Individual model predictions\n",
        "    plt.subplot(131)\n",
        "    plt.bar(probabilities.keys(), [p['probability'] for p in probabilities.values()])\n",
        "    plt.title('Model Predictions')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    # Plot 2: Risk score gauge\n",
        "    plt.subplot(132)\n",
        "    colors = ['green', 'yellow', 'red']\n",
        "    plt.pie([1], colors=[colors[int(final_risk_score * 2)]], \n",
        "           labels=[f'Risk Score: {final_risk_score:.2f}'])\n",
        "    plt.title(f'Final Risk Assessment ({age_group})')\n",
        "    \n",
        "    # Plot 3: URL features\n",
        "    plt.subplot(133)\n",
        "    features = extract_url_features(url)\n",
        "    selected_features = {\n",
        "        'HTTPS': features.get('has_https', 0),\n",
        "        'Suspicious Words': features.get('suspicious_word_count', 0),\n",
        "        'Suspicious TLD': features.get('suspicious_tld', 0),\n",
        "        'IP Address': features.get('is_ip_address', 0)\n",
        "    }\n",
        "    plt.bar(selected_features.keys(), selected_features.values())\n",
        "    plt.title('URL Features')\n",
        "    plt.ylim(0, 1)\n",
        "    plt.xticks(rotation=45)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Test URLs\n",
        "test_urls = [\n",
        "    # Safe URLs\n",
        "    \"https://www.wikipedia.org/wiki/Machine_learning\",\n",
        "    \"https://www.python.org/downloads/\",\n",
        "    \"https://github.com/features\",\n",
        "    \n",
        "    # Potentially unsafe URLs\n",
        "    \"http://suspicious-site.xyz/download.exe\",\n",
        "    \"http://192.168.1.1/admin/hack.php\",\n",
        "    \"http://free-casino-games.tk/poker\",\n",
        "]\n",
        "\n",
        "for url in test_urls:\n",
        "    print(f\"\\nTesting URL: {url}\")\n",
        "    is_unsafe, probability, risk_score = predict_url(url)\n",
        "    print(f\"Is Unsafe: {is_unsafe}\")\n",
        "    print(f\"Probability: {probability:.4f}\")\n",
        "    print(f\"Risk Score: {risk_score:.4f}\")\n",
        "    print(\"-\" * 40)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Conclusion\n",
        "This notebook demonstrates a safe browsing system for kids using machine learning and deep learning. Future improvements include expanding the dataset and adding more sophisticated image detection models."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}