{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# URL Classification Testing\n",
    "\n",
    "This notebook contains both training and testing of the URL classification models with visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.utils import resample\n",
    "import joblib\n",
    "import os\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from dataset import generate_dataset, extract_url_features\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn')\n",
    "sns.set_palette('husl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Generation and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_dataset(X, y):\n",
    "    \"\"\"Balance dataset using upsampling for minority class\"\"\"\n",
    "    X_df = pd.DataFrame(X)\n",
    "    X_df['target'] = y\n",
    "\n",
    "    # Separate majority and minority classes\n",
    "    majority = X_df[X_df.target == 0]\n",
    "    minority = X_df[X_df.target == 1]\n",
    "\n",
    "    # Upsample minority class\n",
    "    minority_upsampled = resample(minority,\n",
    "                                  replace=True,\n",
    "                                  n_samples=len(majority),\n",
    "                                  random_state=42)\n",
    "\n",
    "    # Combine majority class with upsampled minority class\n",
    "    df_balanced = pd.concat([majority, minority_upsampled])\n",
    "\n",
    "    # Separate features and target\n",
    "    y_balanced = df_balanced.target\n",
    "    X_balanced = df_balanced.drop('target', axis=1)\n",
    "\n",
    "    return X_balanced.values, y_balanced.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "print(\"Generating dataset...\")\n",
    "dataset = generate_dataset()\n",
    "\n",
    "# Extract features and labels\n",
    "feature_columns = [col for col in dataset.columns if col not in ['url', 'is_blocked', 'category']]\n",
    "X = dataset[feature_columns].values\n",
    "y = dataset['is_blocked'].values\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Balance training data\n",
    "X_train_balanced, y_train_balanced = balance_dataset(X_train, y_train)\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_balanced)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, model_name):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title(f'Confusion Matrix - {model_name}')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.show()\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_name):\n",
    "    \"\"\"Evaluate model and plot metrics\"\"\"\n",
    "    metrics = {\n",
    "        'Accuracy': accuracy_score(y_true, y_pred),\n",
    "        'Precision': precision_score(y_true, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_true, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_true, y_pred, zero_division=0)\n",
    "    }\n",
    "    \n",
    "    # Plot metrics\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(metrics.keys(), metrics.values())\n",
    "    plt.title(f'Performance Metrics - {model_name}')\n",
    "    plt.ylim(0, 1)\n",
    "    for i, v in enumerate(metrics.values()):\n",
    "        plt.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plot_confusion_matrix(y_true, y_pred, model_name)\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train models\n",
    "models = {\n",
    "    'KNN': KNeighborsClassifier(n_neighbors=5, weights='distance'),\n",
    "    'SVM': SVC(kernel='rbf', probability=True, C=1.0),\n",
    "    'Naive Bayes': GaussianNB()\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "model_predictions = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name} model...\")\n",
    "    model.fit(X_train_scaled, y_train_balanced)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    model_predictions[name] = y_pred\n",
    "    \n",
    "    # Evaluate and plot\n",
    "    print(f\"\\n{name} Model Performance:\")\n",
    "    metrics = evaluate_model(y_test, y_pred, name)\n",
    "    \n",
    "    trained_models[name] = model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Importance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For SVM, we'll use the absolute values of the coefficients as feature importance\n",
    "if hasattr(trained_models['SVM'], 'coef_'):\n",
    "    importances = np.abs(trained_models['SVM'].coef_[0])\n",
    "    feature_imp = pd.DataFrame({'feature': feature_columns, 'importance': importances})\n",
    "    feature_imp = feature_imp.sort_values('importance', ascending=False)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=feature_imp, x='importance', y='feature')\n",
    "    plt.title('Feature Importance (SVM)')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare model performance\n",
    "metrics_df = pd.DataFrame()\n",
    "\n",
    "for name in models.keys():\n",
    "    y_pred = model_predictions[name]\n",
    "    metrics = {\n",
    "        'Model': name,\n",
    "        'Accuracy': accuracy_score(y_test, y_pred),\n",
    "        'Precision': precision_score(y_test, y_pred, zero_division=0),\n",
    "        'Recall': recall_score(y_test, y_pred, zero_division=0),\n",
    "        'F1 Score': f1_score(y_test, y_pred, zero_division=0)\n",
    "    }\n",
    "    metrics_df = pd.concat([metrics_df, pd.DataFrame([metrics])], ignore_index=True)\n",
    "\n",
    "# Plot comparison\n",
    "metrics_melted = pd.melt(metrics_df, \n",
    "                        id_vars=['Model'], \n",
    "                        value_vars=['Accuracy', 'Precision', 'Recall', 'F1 Score'],\n",
    "                        var_name='Metric', \n",
    "                        value_name='Score')\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(data=metrics_melted, x='Model', y='Score', hue='Metric')\n",
    "plt.title('Model Comparison')\n",
    "plt.ylim(0, 1)\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and preprocessing objects\n",
    "save_dir = 'models/latest'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "for name, model in trained_models.items():\n",
    "    filename = name.lower().replace(' ', '_')\n",
    "    joblib.dump(model, os.path.join(save_dir, f'{filename}_model.pkl'))\n",
    "\n",
    "# Save scaler and feature columns\n",
    "joblib.dump(scaler, os.path.join(save_dir, 'url_scaler.pkl'))\n",
    "joblib.dump(feature_columns, os.path.join(save_dir, 'feature_cols.pkl'))\n",
    "\n",
    "print(\"Models saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Test URL Classification with Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_url(url, threshold=0.65, models_dir=None, age_group='kid'):\n",
    "    \"\"\"Make prediction for a single URL using ensemble of models\"\"\"\n",
    "    if models_dir is None:\n",
    "        models_dir = os.path.join(os.path.dirname(os.path.abspath('__file__')), 'models', 'latest')\n",
    "\n",
    "    # Load models and scaler\n",
    "    knn = joblib.load(os.path.join(models_dir, 'knn_model.pkl'))\n",
    "    svm = joblib.load(os.path.join(models_dir, 'svm_model.pkl'))\n",
    "    nb = joblib.load(os.path.join(models_dir, 'naive_bayes_model.pkl'))\n",
    "    scaler = joblib.load(os.path.join(models_dir, 'url_scaler.pkl'))\n",
    "    feature_cols = joblib.load(os.path.join(models_dir, 'feature_cols.pkl'))\n",
    "\n",
    "    # Extract and prepare features\n",
    "    features = extract_url_features(url)\n",
    "    if isinstance(features, dict):\n",
    "        features = list(features.values())\n",
    "    features_array = np.array(features).reshape(1, -1)\n",
    "    features_scaled = scaler.transform(features_array)\n",
    "\n",
    "    # Get predictions\n",
    "    predictions = {}\n",
    "    probas = []\n",
    "\n",
    "    # KNN\n",
    "    knn_prob = knn.predict_proba(features_scaled)[0][1]\n",
    "    predictions['knn'] = {\n",
    "        'prediction': knn_prob > threshold,\n",
    "        'probability': knn_prob\n",
    "    }\n",
    "    probas.append(knn_prob)\n",
    "\n",
    "    # SVM\n",
    "    svm_prob = svm.predict_proba(features_scaled)[0][1]\n",
    "    predictions['svm'] = {\n",
    "        'prediction': svm_prob > threshold,\n",
    "        'probability': svm_prob\n",
    "    }\n",
    "    probas.append(svm_prob)\n",
    "\n",
    "    # Naive Bayes\n",
    "    nb_prob = nb.predict_proba(features_scaled)[0][1]\n",
    "    predictions['nb'] = {\n",
    "        'prediction': nb_prob > threshold,\n",
    "        'probability': nb_prob\n",
    "    }\n",
    "    probas.append(nb_prob)\n",
    "\n",
    "    # Calculate base risk from model predictions\n",
    "    base_features = extract_url_features(url) if isinstance(url, str) else dict(zip(feature_cols, features))\n",
    "\n",
    "    # Trust factors (reduce risk score)\n",
    "    trust_score = 1.0\n",
    "    if base_features.get('has_https', 0) == 1:\n",
    "        trust_score *= 0.7  # Significant trust for HTTPS\n",
    "\n",
    "    # Check for trusted domains\n",
    "    domain = urlparse(url).netloc.lower()\n",
    "    trusted_domains = {'github.com', 'python.org', 'wikipedia.org'}\n",
    "    if any(td in domain for td in trusted_domains):\n",
    "        trust_score *= 0.5  # High trust for known good domains\n",
    "\n",
    "    # Risk factors (increase risk score)\n",
    "    risk_multiplier = 1.0\n",
    "    if base_features.get('is_ip_address', 0) == 1:\n",
    "        risk_multiplier *= 2.0  # Major increase for IP-based URLs\n",
    "    if base_features.get('suspicious_word_count', 0) > 2:\n",
    "        risk_multiplier *= 1.5  # Increase for multiple suspicious words\n",
    "    if base_features.get('suspicious_tld', 0) == 1:\n",
    "        risk_multiplier *= 1.8  # Increase for suspicious TLDs\n",
    "\n",
    "    # Calculate age-specific thresholds\n",
    "    age_thresholds = {\n",
    "        'kid': 0.5,    # More strict for kids\n",
    "        'teen': 0.6,   # Moderate for teens\n",
    "        'adult': 0.7   # More lenient for adults\n",
    "    }\n",
    "    effective_threshold = age_thresholds.get(age_group, threshold)\n",
    "\n",
    "    # Get base risk score from models\n",
    "    risk_level, risk_score = calculate_age_based_risk(\n",
    "        predictions,\n",
    "        base_features,\n",
    "        age_group\n",
    "    )\n",
    "\n",
    "    # Apply trust and risk modifiers\n",
    "    final_risk_score = (risk_score * risk_multiplier * trust_score)\n",
    "\n",
    "    # Ensure score stays in [0,1] range\n",
    "    final_risk_score = max(0.0, min(1.0, final_risk_score))\n",
    "\n",
    "    # Update risk level based on final score\n",
    "    if final_risk_score > 0.8:\n",
    "        risk_level = 'high'\n",
    "    elif final_risk_score > 0.5:\n",
    "        risk_level = 'medium'\n",
    "    else:\n",
    "        risk_level = 'low'\n",
    "\n",
    "    # Enhanced result with age-specific risk assessment\n",
    "    result = {\n",
    "        'is_unsafe': bool(final_risk_score > effective_threshold),\n",
    "        'risk_score': final_risk_score,\n",
    "        'risk_level': risk_level,\n",
    "        'age_group': age_group,\n",
    "        'model_predictions': predictions\n",
    "    }\n",
    "\n",
    "    # Visualize the predictions\n",
    "    visualize_risk_assessment(url, predictions, final_risk_score, age_group)\n",
    "\n",
    "    return result['is_unsafe'], result['risk_score'], result['risk_score']\n",
    "\n",
    "def visualize_risk_assessment(url, probabilities, final_risk_score, age_group):\n",
    "    \"\"\"Create detailed visualization of URL risk assessment\"\"\"\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot 1: Individual model predictions\n",
    "    plt.subplot(131)\n",
    "    plt.bar(probabilities.keys(), [p['probability'] for p in probabilities.values()])\n",
    "    plt.title('Model Predictions')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot 2: Risk score gauge\n",
    "    plt.subplot(132)\n",
    "    colors = ['green', 'yellow', 'red']\n",
    "    plt.pie([1], colors=[colors[int(final_risk_score * 2)]], \n",
    "           labels=[f'Risk Score: {final_risk_score:.2f}'])\n",
    "    plt.title(f'Final Risk Assessment ({age_group})')\n",
    "    \n",
    "    # Plot 3: URL features\n",
    "    plt.subplot(133)\n",
    "    features = extract_url_features(url)\n",
    "    selected_features = {\n",
    "        'HTTPS': features.get('has_https', 0),\n",
    "        'Suspicious Words': features.get('suspicious_word_count', 0),\n",
    "        'Suspicious TLD': features.get('suspicious_tld', 0),\n",
    "        'IP Address': features.get('is_ip_address', 0)\n",
    "    }\n",
    "    plt.bar(selected_features.keys(), selected_features.values())\n",
    "    plt.title('URL Features')\n",
    "    plt.ylim(0, 1)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Test URLs\n",
    "test_urls = [\n",
    "    # Safe URLs\n",
    "    \"https://www.wikipedia.org/wiki/Machine_learning\",\n",
    "    \"https://www.python.org/downloads/\",\n",
    "    \"https://github.com/features\",\n",
    "    \n",
    "    # Potentially unsafe URLs\n",
    "    \"http://suspicious-site.xyz/download.exe\",\n",
    "    \"http://192.168.1.1/admin/hack.php\",\n",
    "    \"http://free-casino-games.tk/poker\",\n",
    "]\n",
    "\n",
    "for url in test_urls:\n",
    "    print(f\"\\nTesting URL: {url}\")\n",
    "    is_unsafe, probability, risk_score = predict_url(url)\n",
    "    print(f\"Is Unsafe: {is_unsafe}\")\n",
    "    print(f\"Probability: {probability:.4f}\")\n",
    "    print(f\"Risk Score: {risk_score:.4f}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
